{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading an image \n",
    "input = cv2.imread('download.jpg')\n",
    "cv2.imshow('Hello All',input)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = cv2.imread('download.jpg')\n",
    "cv2.imshow('img', input)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyWindow('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GreyScale Image \n",
    "image = cv2.imread('download.jpg')\n",
    "grey_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('GREYSCALE',grey_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GreyScale Image Faster Method\n",
    "image = cv2.imread('download.jpg',0)\n",
    "#grey_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('GREYSCALE',image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Histogram are great way to visualize individual color components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = cv2.imread('tobago.jpg')\n",
    "histogram = cv2.calcHist([images],[0], None, [256],[0,256])\n",
    "# We plot a histogram, ravel() flatens our image array\n",
    "plt.hist(images.ravel(),256,[0,256]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])\n",
    "\n",
    "  - images : it is the source image of type uint8 or float32. it should be given in square brackets, ie, \"[img]\".\n",
    "  - channels : it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0], [1] or [2] to calculate histogram of blue, green or red channel respectively.\n",
    "  - mask : mask image. To find histogram of full image, it is given as \"None\". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)\n",
    "  - histSize : this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].\n",
    "  - ranges : this is our RANGE. Normally, it is [0,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Viewing Seperate Color Channels \n",
    "color = ('b','g','r')\n",
    "\n",
    "#We now seperate the colors and plot each in the Histogram \n",
    "for i,col in enumerate(color):\n",
    "    histogram2 = cv2.calcHist([images],[i],None,[256],[0,256])\n",
    "    plt.plot(histogram2, color=col)\n",
    "    plt.xlim([0,256])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('tobago.jpg')\n",
    "cv2.imshow(\"Tobago\", image) \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing images and shapes using OpenCV\n",
    "Let's start off my making a black square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "#create a black image \n",
    "image = np.zeros((512,512,3),np.uint8)\n",
    "\n",
    "#can we make this in black and white?\n",
    "image1= np.zeros((512,512), np.uint8)\n",
    "\n",
    "cv2.imshow(\"Black Rectangle (Color)\",image) \n",
    "cv2.imshow(\"Black Rectangle (B&W)\", image1)\n",
    "#Both the images are identical\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's draw a line over our black sqare\n",
    "cv2.line(image, starting cordinates, ending cordinates, color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a diagonal blue of thickness of 5 pixels \n",
    "image = np.zeros((512,512,3),np.uint8)\n",
    "cv2.line(image,(512,0),(0,512),(255,177,0),5)\n",
    "cv2.imshow(\"Blue Line\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's now draw a rectangle\n",
    "cv2.rectangle(image, starting vertex, opposite vertex, color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a rectangle \n",
    "image = np.zeros((512,512,3),np.uint8)\n",
    "cv2.rectangle(image,(100,100),(300,250),(255,177,0),5)\n",
    "cv2.imshow(\"Blue Line\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a rectangle \n",
    "image = np.zeros((512,512,3),np.uint8)\n",
    "cv2.rectangle(image,(100,100),(300,250),(255,177,0),-1) ## -1 fills the rectangle \n",
    "cv2.imshow(\"Blue Line\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about cirlcles?\n",
    "cv2.cirlce(image, center, radius, color, fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a circle\n",
    "image = np.zeros((512,512,3),np.uint8)\n",
    "cv2.circle(image,(206,206),200,(255,177,0),-1) ## -1 fills the rectangle \n",
    "cv2.imshow(\"Blue Line\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And polygons?\n",
    "cv2.polylines(image, [pts], isClosed, color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = np.zeros((512,512,3),np.uint8)\n",
    "#Defining the four points\n",
    "points = np.array([[10,50],[400,50],[90,200],[50,200]], np.int32)\n",
    "#Reshape the points in form  required by polylines\n",
    "points = points.reshape((-1,1,2))\n",
    "\n",
    "cv2.polylines(image,[points],True,(0,0,255),3)\n",
    "cv2.imshow(\"Polygon\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's even add text with cv2.putText\n",
    "cv2.putText(image, 'Text to Display', bottom left starting point, Font, Font Size, Color, Thickness)\n",
    "\n",
    "Different fonts available are:\n",
    " - FONT_HERSHEY_SIMPLEX\n",
    " - FONT_HERSHEY_PLAIN \n",
    " - FONT_HERSHEY_DUPLEX\n",
    " - FONT_HERSHEY_COMPLEX\n",
    " - FONT_HERSHEY_TRIPLEX\n",
    " - FONT_HERSHEY_COMPLEX_SMALL\n",
    " - FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    " - FONT_HERSHEY_SCRIPT_COMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image=np.zeros((512,512,3),np.uint8)\n",
    "cv2.putText(image,\"Hello Rahul\",(70,290),cv2.FONT_HERSHEY_PLAIN,2,(100,170,3),3)\n",
    "cv2.imshow(\"Hello Rahul\",image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translations\n",
    "This an affine transform that simply shifts the position of an image.\n",
    "\n",
    "We use cv2.warpAffine to implement these transformations.\n",
    "\n",
    "Syntax: cv2.warpAffine(src, M, dsize, dst, flags, borderMode, borderValue)\n",
    "\n",
    "Parameters:\n",
    "- src: input image.\n",
    "- dst: output image that has the size dsize and the same type as src.\n",
    "- M: transformation matrix.\n",
    "- dsize: size of the output image.\n",
    "- flags: combination of interpolation methods (see resize() ) and the optional flag\n",
    "  WARP_INVERSE_MAP that means that M is the inverse transformation (dst->src).\n",
    "- borderMode: pixel extrapolation method; when borderMode=BORDER_TRANSPARENT, it means that the pixels in the     destination image corresponding to the “outliers” in the source image are not modified by the function.\n",
    "- borderValue: value used in case of a constant border; by default, it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "#store the height and width\n",
    "height,width = image.shape[:2]\n",
    "quarter_height,quarter_width = height/4,width/4\n",
    "\n",
    "#       | 1 0 Tx |\n",
    "#  T  = | 0 1 Ty |\n",
    "#T-translation matrix\n",
    "T=np.array([[1,0,quarter_width],[0,1,quarter_height]],np.float32)\n",
    "\n",
    "image_translation = cv2.warpAffine(image,T,(width,height))\n",
    "cv2.imshow(\"Translation\",image_translation)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotations\n",
    "cv2.getRotationMatrix2D((rotation_center_x, rotation_center_y), angle of rotation, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('input.jpg')\n",
    "height,width = image.shape[:2]\n",
    "\n",
    "#Rotation matrix \n",
    "rotation_matrix = cv2.getRotationMatrix2D((width/2,height/2),90,0.5)\n",
    "\n",
    "rotate_image = cv2.warpAffine(image,rotation_matrix,(width,height))\n",
    "cv2.imshow(\"Rotation Matrix\",rotate_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other Option to rotate 90 only\n",
    "image = cv2.imread('input.jpg')\n",
    "\n",
    "rotated_image = cv2.transpose(image)\n",
    "\n",
    "cv2.imshow(\"Rotation image\",rotated_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax: cv2.cv.flip(src, flipCode[, dst] )\n",
    "\n",
    "Parameters:\n",
    "- src: Input array.\n",
    "- dst: Output array of the same size and type as src.\n",
    "- flip code: A flag to specify how to flip the array; 0 means flipping around the x-axis and positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means flipping around both axes.\n",
    "\n",
    "Return Value: It returns an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Horizontal Flip\n",
    "flipped = cv2.flip(image,1)\n",
    "cv2.imshow(\"Flipped Image\", flipped)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling, re-sizing and interpolations\n",
    "Re-sizing is very easy using the cv2.resize function, it's arguments are:\n",
    "\n",
    "cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Load the image \n",
    "image = cv2.imread('input.jpg')\n",
    "\n",
    "#Resizing it to 3/4 of it's original size\n",
    "image_scaled = cv2.resize(image, None, fx=0.75, fy=1)\n",
    "cv2.imshow(\"Rescaled_image - Linear Interpolation\", image_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Double the size of the original image\n",
    "image_doubled = cv2.resize(image,None, fx=2, fy=2, interpolation= cv2.INTER_CUBIC)\n",
    "cv2.imshow(\"Doubled_image - Cubic Interpolation\", image_doubled)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Skew the resizing by setting exact dimension\n",
    "image_skewed = cv2.resize(image,(900,400), interpolation= cv2.INTER_AREA)\n",
    "cv2.imshow(\"Scaling - skewed image\", image_skewed)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Pyramids\n",
    "Useful when scaling images in object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "smaller = cv2.pyrDown(image) ##cv2.pyrDown reduces the image by 50%\n",
    "larger = cv2.pyrUp(smaller) ##cv2.pyrUp doubles the image size\n",
    "\n",
    "cv2.imshow(\"Smaller Inmage\", smaller)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.imshow(\"Larger Image\", larger)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "height, width = image.shape[:2]\n",
    "# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n",
    "start_row, start_col = int(height*0.25), int(width*0.25)\n",
    "# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n",
    "end_row, end_col = int(height*0.75), int(width*0.75)\n",
    "# Simply use indexing to crop out the rectangle we desire\n",
    "cropped_image= image[start_row:end_row,start_col:end_col]\n",
    "\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.waitKey()\n",
    "cv2.imshow(\"Cropped Image\", cropped_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic Operations\n",
    "These are simple operations that allow us to directly add or subract to the color intensity.\n",
    "\n",
    "Calculates the per-element operation of two arrays. The overall effect is increasing or decreasing brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "\n",
    "M = np.ones(image.shape, np.uint8)*75\n",
    "\n",
    "added = cv2.add(image, M)\n",
    "cv2.imshow(\"Added\", added)\n",
    "cv2.waitKey()\n",
    "\n",
    "subtracted = cv2.subtract(image,M)\n",
    "cv2.imshow(\"Substracted\", subtracted)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitwise Operations and Masking¶\n",
    "To demonstrate these operations let's create some simple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "square = np.zeros([300,300], np.uint8)\n",
    "cv2.rectangle(square,(50,50),(250,250),255, -1)\n",
    "cv2.imshow(\"square\",square)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Making an ellipse \n",
    "ellipse = np.zeros([300,300], np.uint8)\n",
    "cv2.ellipse(ellipse,(150,150),(250,250),30,0,180,255, -1)\n",
    "cv2.imshow(\"ellipse\",ellipse)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with some bitwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shows only where they intersect\n",
    "And = cv2.bitwise_and(square,ellipse)\n",
    "cv2.imshow(\"AND\", And)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Shows where either square or ellipse is\n",
    "OR = cv2.bitwise_or(square,ellipse)\n",
    "cv2.imshow(\"OR\",OR)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Shows where either exist by itself\n",
    "bitwiseXOR = cv2.bitwise_xor(square,ellipse)\n",
    "cv2.imshow(\"XOR\", bitwiseXOR)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Shows everything that isn't part of the square\n",
    "bitwiseNot_sq = cv2.bitwise_not(square)\n",
    "cv2.imshow(\"NOT- Square\",bitwiseNot_sq)\n",
    "cv2.waitKey()\n",
    "\n",
    "### Notice the last operation inverts the image totally\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions & Blurring\n",
    "A Convolution is a mathematical operation performed on two functions producing a third function which is typically a modified version of one of the original functions.\n",
    "Output Image = Image * Function(of kernal size)\n",
    "\n",
    "In Computer Vision we use kernel’s to specify the size over which we run our manipulating function over our image.\n",
    "\n",
    "<img src = \"1.png\">\n",
    "\n",
    "All the filters like Median, Gaussian are domain filters. This means that the filter weights are assigned using the spatial closeness (i.e. domain). This has an issue as it will blur the edges also. Domain filters doesn’t consider whether a pixel is an edge pixel or not. It just assigns weights according to spatial closeness and thus leads to edge blurring. Range filtering alone also doesn’t solve the problem of edge blurring. \n",
    "\n",
    "If we combine both domain and range filtering. This will solve the problem of edge blurring.(Bilateral Blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Creating a 3X3 kernel\n",
    "kernel_3x3= np.ones((3,3), np.float32)/9\n",
    "\n",
    "#We use the cv2.filter2D to convolve the kernel with an image\n",
    "blurred = cv2.filter2D(image, -1, kernel_3x3)\n",
    "cv2.imshow(\"3x3 kernel Blurring\", blurred)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Creating a 7X7 kernel\n",
    "kernel_7x7 = np.ones((7,7),np.float32)/49\n",
    "\n",
    "blurred2 = cv2.filter2D(image,-1,kernel_7x7)\n",
    "cv2.imshow(\"7x7 kernel blurring\", blurred2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other commonly used blurring methods in OpenCV\n",
    "Syntax: cv2.blur(src, ksize[, dst[, anchor[, borderType]]])\n",
    "\n",
    "Parameters:\n",
    "- src: It is the image whose is to be blurred.\n",
    "- ksize: A tuple representing the blurring kernel size.\n",
    "- dst: It is the output image of the same size and type as src.\n",
    "- anchor: It is a variable of type integer representing anchor point and it’s default value Point is (-1, -1) which means that the anchor is at the kernel center.\n",
    "- borderType: It depicts what kind of border to be added. It is defined by flags like cv2.BORDER_CONSTANT, cv2.BORDER_REFLECT, etc\n",
    "Return Value: It returns an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "# Averaging done by convolving the image with a normalized box filter. \n",
    "# This takes the pixels under the box and replaces the central element\n",
    "# Box size needs to odd and positive \n",
    "blur = cv2.blur(image,(3,3))\n",
    "cv2.imshow(\"Blurred Image\", blur)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType=BORDER_DEFAULT]]])\n",
    "Parameter\tDescription\n",
    "- src\tinput image\n",
    "- dst\toutput image\n",
    "- ksize\tGaussian Kernel Size. [height width]. height and width should be odd and can have different values. If ksize is set to [0 0], then ksize is computed from sigma values.\n",
    "- sigmaX\tKernel standard deviation along X-axis (horizontal direction).\n",
    "- sigmaY\tKernel standard deviation along Y-axis (vertical direction). If sigmaY=0, then sigmaX value is taken for sigmaY\n",
    "- borderType\tSpecifies image boundaries while kernel is applied on image borders. Possible values are : cv.BORDER_CONSTANT cv.BORDER_REPLICATE cv.BORDER_REFLECT cv.BORDER_WRAP cv.BORDER_REFLECT_101 cv.BORDER_TRANSPARENT cv.BORDER_REFLECT101 cv.BORDER_DEFAULT cv.BORDER_ISOLATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instead of box filter, gaussian kernel\n",
    "Gaussian = cv2.GaussianBlur(image,(7,7),0)\n",
    "cv2.imshow(\"Gaussian Blur\", Gaussian)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv2.medianBlur(input_image, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes median of all the pixels under kernel area and central \n",
    "# element is replaced with this median value\n",
    "median = cv2.medianBlur(image,5)\n",
    "cv2.imshow(\"Median Blur\", median)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]]) → dst\n",
    "Parameters:\t\n",
    "- src – Source 8-bit or floating-point, 1-channel or 3-channel image.\n",
    "- dst – Destination image of the same size and type as src .\n",
    "- d – Diameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace .\n",
    "- sigmaColor – Filter sigma in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace ) will be mixed together, resulting in larger areas of semi-equal color.\n",
    "- sigmaSpace – Filter sigma in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough (see sigmaColor ). When d>0 , it specifies the neighborhood size regardless of sigmaSpace . Otherwise, d is proportional to sigmaSpace .\n",
    "The function applies bilateral filtering to the input image, as described in http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html bilateralFilter can reduce unwanted noise very well while keeping edges fairly sharp. However, it is very slow compared to most filters.\n",
    "\n",
    "Sigma values: For simplicity, you can set the 2 sigma values to be the same. If they are small (< 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very strong effect, making the image look “cartoonish”.\n",
    "\n",
    "Filter size: Large filters (d > 5) are very slow, so it is recommended to use d=5 for real-time applications, and perhaps d=9 for offline applications that need heavy noise filtering.\n",
    "\n",
    "This filter does not work inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bilateral is very effective in noise removal while keeping edges sharp\n",
    "bilateral = cv2.bilateralFilter(image,9,75,75)\n",
    "cv2.imshow(\"Bilateral blur\", bilateral)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpening\n",
    "By altering our kernels we can implement sharpening, which has the effects of in strengthening or emphasizing edges in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "cv2.imshow(\"Original Image\",image)\n",
    "cv2.waitKey()\n",
    "\n",
    "kernel_sharpening = np.array([[-1,-1,-1],\n",
    "                             [-1,9,-1],\n",
    "                             [-1,-1,-1]])\n",
    "\n",
    "sharpened = cv2.filter2D(image,-1,kernel_sharpening)\n",
    "cv2.imshow(\"Image Sharpening\", sharpened)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding, Binarization & Adaptive Thresholding¶\n",
    "In thresholding, we convert a grey scale image to it's binary form\n",
    "\n",
    "cv2.threshold(image,Threshold Value, Max Value, Threshols Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('gradient.jpg')\n",
    "cv2.imshow(\"Original Image\",image)\n",
    "\n",
    "ret,thresh1 = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "cv2.imshow(\"1 Threshold Binary\", thresh1)\n",
    "\n",
    "ret,thresh2 = cv2.threshold(image, 127,255,cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow(\"2 Threshold Binary Inverse\", thresh2)\n",
    "\n",
    "ret,thresh3 = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
    "cv2.imshow(\"3 Threshold Trunc\", thresh3)\n",
    "\n",
    "ret,thresh4 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
    "cv2.imshow(\"4 Threshold TOZERO\",thresh4)\n",
    "\n",
    "ret,thresh5 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "cv2.imshow(\"5 Threshold TOZERO_INV\",thresh5)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a better way off thresholding?**\n",
    "\n",
    "The biggest downfall of those simple threshold methods is that we need to provide the threshold value (i.e. the 127 value we used previously).\n",
    "\n",
    "**What if there was a smarter way of doing this?**\n",
    "\n",
    "There is with, Adaptive thresholding.\n",
    "\n",
    "_cv2.adaptiveThreshold(image, Max Value, Adaptive Type, Threshold Type, Block Size, Constant that is substracted from mean)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('Origin_of_Species.jpg')\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.waitKey()\n",
    "#converting image to gray_scale\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Applying threshold \n",
    "_, thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow(\"Threshold Binary\", thresh1)\n",
    "\n",
    "#It is a good practice to blur images as it removes noise\n",
    "image = cv2.GaussianBlur(image,(3,3),0)\n",
    "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5) \n",
    "#thresh = cv2.adaptiveThreshold(image,255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5)\n",
    "cv2.imshow(\"Adaptive Mean Thresholding\", thresh)\n",
    "cv2.waitKey()\n",
    "\n",
    "_,thresh2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "cv2.imshow(\"OTSU Thresholding\", thresh2)\n",
    "cv2.waitKey()\n",
    "\n",
    "image = cv2.GaussianBlur(image,(5,5),0)\n",
    "_,thresh3 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "cv2.imshow(\"OTSU Thresholding\", thresh2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilation, Erosion, Opening and Closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('opencv_inv.jpg')\n",
    "\n",
    "cv2.imshow(\"original image\", image)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Defining Kernel matrix size\n",
    "kernel_size = np.ones((5,5),np.uint8)\n",
    "\n",
    "#Erosion- Removing pixels from the outline of an object\n",
    "erosion =cv2.erode(image,kernel_size, iterations=1)\n",
    "cv2.imshow('Erosion',erosion)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Dilation- Adding pixels to the outline of an object\n",
    "dilation = cv2.dilate(image,kernel_size,iterations=1)\n",
    "cv2.imshow('Dilation', dilation)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Opening - Good for removing noise\n",
    "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel_size)\n",
    "cv2.imshow('Opening', opening)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Closing - Good for removing noise\n",
    "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel_size)\n",
    "cv2.imshow('Closing', closing)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There are some other less popular morphology operations, see the official OpenCV site:__\n",
    "http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection & Image Gradients\n",
    "\n",
    "There are three main types of Edge Detection:\n",
    "\n",
    "- Sobel– to emphasize vertical or horizontal edges\n",
    "\n",
    "- Laplacian Laplacian – Gets all orientations\n",
    "\n",
    "- Canny– Optimal due to low error rate, well defined edges and accurate detection.\n",
    "\n",
    "**Canny Edge Detection Algorithm** (developed by John F. Canny in 1986)\n",
    "1. Applies Gaussian blurring\n",
    "2. Finds intensity gradient of the image\n",
    "3. Applied non-maximum suppression (i.e. removes pixels that are not edges)\n",
    "4. Hysteresis – Applies thresholds (i.e. if pixel is within the upper and lower\n",
    "thresholds, it is considered an edge)\n",
    "\n",
    "\n",
    "Learn more here – https://en.wikipedia.org/wiki/Canny_edge_detector\n",
    "\n",
    "Comparison of Edge Detection Techniques - http://www.ijcsi.org/papers/IJCSI-9-5-1-269-276.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#Reading the image in grayscale mode\n",
    "image = cv2.imread('input.jpg',0)\n",
    "height, width = image.shape\n",
    "\n",
    "#Extract Sobel Edges\n",
    "sobel_x = cv2.Sobel(image, cv2.CV_64F,0,1,ksize=5)\n",
    "sobel_y = cv2.Sobel(image, cv2.CV_64F,1,0,ksize=5)\n",
    "\n",
    "cv2.imshow('Original Image',image)\n",
    "cv2.waitKey()\n",
    "cv2.imshow('Sobel_X', sobel_x)\n",
    "cv2.waitKey()\n",
    "cv2.imshow('Sobel_Y', sobel_y)\n",
    "cv2.waitKey()\n",
    "\n",
    "sobel_OR = cv2.bitwise_or(sobel_x,sobel_y)\n",
    "cv2.imshow('sobel_OR',sobel_OR)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Laplacian \n",
    "laplace = cv2.Laplacian(image,cv2.CV_64F)\n",
    "cv2.imshow('Laplacian', laplace)\n",
    "cv2.waitKey()\n",
    "\n",
    "\n",
    "##  Then, we need to provide two values: threshold1 and threshold2. Any gradient value larger than threshold2\n",
    "# is considered to be an edge. Any value below threshold1 is considered not to be an edge. \n",
    "#Values in between threshold1 and threshold2 are either classiﬁed as edges or non-edges based on how their \n",
    "#intensities are “connected”. In this case, any gradient values below 60 are considered non-edges\n",
    "#whereas any values above 120 are considered edges.\n",
    "\n",
    "\n",
    "# Canny Edge Detection uses gradient values as thresholds\n",
    "# The first threshold gradient\n",
    "canny = cv2.Canny(image, 50, 170)\n",
    "cv2.imshow('Canny', canny)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Perpsective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('scan.jpg')\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Cordinates of the 4 corners of the original image \n",
    "points_A = np.array([[320,15],[700,215],[85,610],[530,780]], np.float32)\n",
    "\n",
    "#Cordinates of the 4 corners of the desired output \n",
    "#We use a ratio of an A4 paper 1:41\n",
    "points_B = np.array([[0,0],[420,0],[0,594],[420,594]], np.float32)\n",
    "\n",
    "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
    "\n",
    "warped = cv2.warpPerspective(image, M, (420,594))\n",
    "\n",
    "cv2.imshow('warpPerspective',warped)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In affine transforms you only need 3 coordiantes to obtain the correct transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('ex2.jpg')\n",
    "rows,cols,_ = image.shape\n",
    "\n",
    "cv2.imshow('original Image', image)\n",
    "cv2.waitKey()\n",
    "\n",
    "#Co-ordinates of the 3 points requires \n",
    "points_A = np.array([[320,15],[700,215],[85,610]], np.float32)\n",
    "\n",
    "points_B = np.float32([[0,0],[420,0],[0,594]])\n",
    "\n",
    "M = cv2.getAffineTransform(points_A, points_B)\n",
    "\n",
    "warped = cv2.warpAffine(image, M,(rows,cols))\n",
    "\n",
    "cv2.imshow('Affine Transformation',warped)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Definig sketch generating feature\n",
    "def sketch(image):\n",
    "    #Convert image to grayscale \n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Clean-up image \n",
    "    img_gray_blur = cv2.GaussianBlur(image,(5,5),0)\n",
    "    \n",
    "    #Extract edge\n",
    "    canny_edges = cv2.Canny(img_gray_blur,10,70)\n",
    "    \n",
    "    #Do an invert binarize the image\n",
    "    _,mask = cv2.threshold(canny_edges,70,255,cv2.THRESH_BINARY_INV)\n",
    "    return mask\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _,frame = cap.read()\n",
    "    cv2.imshow('Sketch Image', sketch(frame))\n",
    "    ##13 - \"Enter\" key\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "\n",
    "#Release Camera and close window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
